{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug Uni-Mol embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 13:46:04 | unimol_tools/models/unimol.py | 146 | INFO | Uni-Mol(QSAR) | Loading pretrained weights from /mnt/USR_DATA/ChenGeng/anaconda3/envs/chemprop/lib/python3.8/site-packages/unimol_tools-1.0.0-py3.8.egg/unimol_tools/weights/mol_pre_no_h_220816.pt\n",
      "Checking existing SMILES: 100%|██████████| 1240/1240 [00:00<00:00, 100981.22it/s]\n",
      "Processing new SMILES in batches:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-25 13:46:14 | unimol_tools/data/conformer.py | 90 | INFO | Uni-Mol(QSAR) | Start generating conformers...\n",
      "1240it [00:12, 101.02it/s]\n",
      "2025-03-25 13:46:27 | unimol_tools/data/conformer.py | 94 | INFO | Uni-Mol(QSAR) | Failed to generate conformers for 0.00% of molecules.\n",
      "2025-03-25 13:46:27 | unimol_tools/data/conformer.py | 96 | INFO | Uni-Mol(QSAR) | Failed to generate 3d conformers for 0.24% of molecules.\n",
      "100%|██████████| 39/39 [00:11<00:00,  3.34it/s]\n",
      "Processing new SMILES in batches: 100%|██████████| 1/1 [02:46<00:00, 166.83s/it]\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sys\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the path of 'unimol_tools' to Python's search path\n",
    "sys.path.append('./Uni-Mol-main/unimol_tools/unimol_tools')\n",
    "from unimol_tools import UniMolRepr\n",
    "\n",
    "# =========================\n",
    "# 1. Database operations\n",
    "# =========================\n",
    "\n",
    "def create_table(conn):\n",
    "    \"\"\"\n",
    "    Create (if it does not exist) the table 'molecule_atomic_embeddings' \n",
    "    in the database to store molecular representations.\n",
    "    :param conn: sqlite3.Connection object.\n",
    "    \"\"\"\n",
    "    conn.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS molecule_atomic_embeddings (\n",
    "            smiles TEXT PRIMARY KEY,\n",
    "            embedding BLOB,\n",
    "            atomic_embedding BLOB\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "def store_molecule_embedding(conn, smiles, embedding, atomic_embedding):\n",
    "    \"\"\"\n",
    "    Store the UniMol CLS-level representation and atomic-level representation\n",
    "    of a molecule into the database.\n",
    "    :param conn: sqlite3.Connection object.\n",
    "    :param smiles: String, SMILES of the molecule.\n",
    "    :param embedding: ndarray, the CLS (global) representation of the molecule.\n",
    "    :param atomic_embedding: ndarray, the atomic-level representations of the molecule.\n",
    "    \"\"\"\n",
    "    embedding_buffer = io.BytesIO()\n",
    "    atomic_buffer = io.BytesIO()\n",
    "\n",
    "    # Save the numpy arrays into in-memory buffers\n",
    "    np.save(embedding_buffer, embedding)\n",
    "    np.save(atomic_buffer, atomic_embedding)\n",
    "    embedding_buffer.seek(0)\n",
    "    atomic_buffer.seek(0)\n",
    "\n",
    "    # Store them in the database as BLOBs\n",
    "    conn.execute('''\n",
    "        INSERT OR REPLACE INTO molecule_atomic_embeddings (smiles, embedding, atomic_embedding)\n",
    "        VALUES (?, ?, ?)\n",
    "    ''', (\n",
    "        smiles,\n",
    "        sqlite3.Binary(embedding_buffer.read()),\n",
    "        sqlite3.Binary(atomic_buffer.read())\n",
    "    ))\n",
    "    conn.commit()\n",
    "\n",
    "def get_molecule_embedding(conn, smiles):\n",
    "    \"\"\"\n",
    "    Retrieve the UniMol CLS-level representation and atomic-level representation\n",
    "    for a given SMILES from the database.\n",
    "    :param conn: sqlite3.Connection object.\n",
    "    :param smiles: String, SMILES of the molecule.\n",
    "    :return: A tuple (embedding, atomic_embedding); returns (None, None) if not found.\n",
    "    \"\"\"\n",
    "    cursor = conn.execute('''\n",
    "        SELECT embedding, atomic_embedding\n",
    "        FROM molecule_atomic_embeddings\n",
    "        WHERE smiles=?\n",
    "    ''', (smiles,))\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    if result:\n",
    "        # result[0] -> embedding, result[1] -> atomic_embedding\n",
    "        return (\n",
    "            np.load(io.BytesIO(result[0]), allow_pickle=True),\n",
    "            np.load(io.BytesIO(result[1]), allow_pickle=True)\n",
    "        )\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# =========================\n",
    "# 2. Representation generation\n",
    "# =========================\n",
    "\n",
    "def process_smiles_batch(smiles_batch):\n",
    "    \"\"\"\n",
    "    Process a batch of SMILES strings, returning a list of (SMILES, CLS representation, atomic representation).\n",
    "    Use only for small batches or when needed; for large datasets, consider a single initialization of UniMolRepr.\n",
    "    :param smiles_batch: List of SMILES strings.\n",
    "    :return: A list of tuples (smiles, cls_repr, atomic_reprs).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This uses CPU mode by default; enable use_gpu=True if needed\n",
    "        clf = UniMolRepr(data_type='molecule', remove_hs=True)\n",
    "        unimol_repr = clf.get_repr(smiles_batch, return_atomic_reprs=True)\n",
    "\n",
    "        results = []\n",
    "        for i, smiles in enumerate(smiles_batch):\n",
    "            cls_repr = np.array(unimol_repr['cls_repr'][i])       # CLS representation\n",
    "            atomic_reprs = np.array(unimol_repr['atomic_reprs'][i])  # Atomic-level representation\n",
    "            results.append((smiles, cls_repr, atomic_reprs))\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process batch: {e}\")\n",
    "        return [None] * len(smiles_batch)\n",
    "\n",
    "def process_smiles_data(conn, smiles_list, batch_size=5000):\n",
    "    \"\"\"\n",
    "    Process the given list of SMILES in batches, generate UniMol representations, \n",
    "    and store them in the database.\n",
    "    :param conn: sqlite3.Connection object.\n",
    "    :param smiles_list: Iterable of SMILES strings.\n",
    "    :param batch_size: Integer, max number of SMILES to process in one batch. Adjust based on available memory.\n",
    "    \"\"\"\n",
    "    # Initialize UniMolRepr once (enable GPU if memory allows)\n",
    "    clf = UniMolRepr(data_type='molecule', remove_hs=True, use_gpu=True)\n",
    "\n",
    "    # Remove duplicates by converting to a set\n",
    "    unique_smiles_set = set(smiles_list)\n",
    "\n",
    "    # Collect SMILES strings not yet in the database\n",
    "    new_smiles_list = []\n",
    "    for smiles in tqdm(unique_smiles_set, desc=\"Checking existing SMILES\"):\n",
    "        # If the SMILES is not in the database, we include it for processing\n",
    "        if get_molecule_embedding(conn, smiles)[0] is None:\n",
    "            new_smiles_list.append(smiles)\n",
    "\n",
    "    # Process and store in batches\n",
    "    for i in tqdm(range(0, len(new_smiles_list), batch_size), desc=\"Processing new SMILES in batches\"):\n",
    "        batch = new_smiles_list[i : i+batch_size]\n",
    "        try:\n",
    "            unimol_repr = clf.get_repr(batch, return_atomic_reprs=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process batch: {e}\")\n",
    "            continue\n",
    "\n",
    "        for j, smiles in enumerate(batch):\n",
    "            cls_repr = np.array(unimol_repr['cls_repr'][j])\n",
    "            atomic_reprs = np.array(unimol_repr['atomic_reprs'][j])\n",
    "            store_molecule_embedding(conn, smiles, cls_repr, atomic_reprs)\n",
    "\n",
    "# =========================\n",
    "# 3. Main script\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect('./data/11betaHSD1/Uni-Mol_molecule_embeddings_no_h_11betaHSD1_ligands.db')\n",
    "    \n",
    "    # Create the table (if it does not exist)\n",
    "    create_table(conn)\n",
    "\n",
    "    # Load SMILES from a CSV file\n",
    "    df = pd.read_csv(\"./data/11betaHSD1/11betaHSD1.csv\")\n",
    "    smiles_data = df[\"SMILES\"].tolist()\n",
    "\n",
    "    # Process SMILES in batches and store to the database\n",
    "    process_smiles_data(conn, smiles_data, batch_size=5000)\n",
    "\n",
    "    # Close the database connection\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ankh Large protein embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing protein sequences: 100%|██████████| 500/500 [00:00<00:00, 1039.77it/s]\n",
      "Processing protein sequences: 100%|██████████| 500/500 [00:00<00:00, 1044.36it/s]\n",
      "Processing protein sequences: 100%|██████████| 240/240 [00:00<00:00, 1060.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1. Imports and Environment Setup\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import io\n",
    "import torch\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoConfig, AutoTokenizer, T5EncoderModel\n",
    "\n",
    "\n",
    "# Select device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# 2. Function Definitions\n",
    "# =========================\n",
    "\n",
    "def read_data_in_chunks(file_path, chunk_size):\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        yield chunk\n",
    "\n",
    "def create_table(conn):\n",
    "    conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS protein_embeddings (\n",
    "        protein_sequence TEXT PRIMARY KEY,\n",
    "        embedding BLOB\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "def store_embedding(conn, protein_sequence, embedding):\n",
    "    buffer = io.BytesIO()\n",
    "    np.save(buffer, embedding.cpu().numpy())\n",
    "    buffer.seek(0)\n",
    "    conn.execute('''\n",
    "    INSERT OR REPLACE INTO protein_embeddings (protein_sequence, embedding)\n",
    "    VALUES (?, ?)\n",
    "    ''', (protein_sequence, sqlite3.Binary(buffer.read())))\n",
    "    conn.commit()\n",
    "\n",
    "def get_protein_embedding(conn, protein_sequence):\n",
    "    cursor = conn.execute('''\n",
    "    SELECT embedding FROM protein_embeddings WHERE protein_sequence=?\n",
    "    ''', (protein_sequence,))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        return np.load(io.BytesIO(result[0]), allow_pickle=True)\n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# 3. Main Execution\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths and parameters\n",
    "    # Path to the pre-trained Ankh model \n",
    "    model_path = \"./model/Ankh_Large/Ankh_Large_model.pth\"\n",
    "    data_path = \"./data/11betaHSD1/11betaHSD1.csv\"\n",
    "    db_path = \"./data/11betaHSD1/Ankh_Large_target_protein_embeddings_11betaHSD1.db\"\n",
    "    max_length = 1200\n",
    "    chunk_size = 500\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = T5EncoderModel.from_pretrained(model_path, config=config)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    create_table(conn)\n",
    "\n",
    "    # Process protein sequences in chunks\n",
    "    for df_chunk in read_data_in_chunks(data_path, chunk_size):\n",
    "        for i in tqdm(range(len(df_chunk)), desc=\"Processing protein sequences\"):\n",
    "            original_seq = df_chunk.iloc[i]['Protein']\n",
    "\n",
    "            if get_protein_embedding(conn, original_seq) is not None:\n",
    "                continue\n",
    "\n",
    "            seq = original_seq[:1200] if len(original_seq) > 1200 else original_seq\n",
    "            tokenized = tokenizer.batch_encode_plus(\n",
    "                [list(seq)],\n",
    "                add_special_tokens=True,\n",
    "                padding=True,\n",
    "                is_split_into_words=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            input_ids = tokenized['input_ids'].to(device)\n",
    "            attention_mask = tokenized['attention_mask'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                encoder_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            token_repr = encoder_outputs.last_hidden_state\n",
    "\n",
    "            if token_repr.shape[1] > max_length:\n",
    "                token_repr = token_repr[:, :max_length, :]\n",
    "\n",
    "            store_embedding(conn, original_seq, token_repr.squeeze(0))\n",
    "\n",
    "            del token_repr, encoder_outputs, input_ids, attention_mask, tokenized, seq, original_seq\n",
    "            gc.collect()\n",
    "\n",
    "    # Close the database\n",
    "    conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
