{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug Uni-Mol embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 13:46:04 | unimol_tools/models/unimol.py | 146 | INFO | Uni-Mol(QSAR) | Loading pretrained weights from /mnt/USR_DATA/ChenGeng/anaconda3/envs/chemprop/lib/python3.8/site-packages/unimol_tools-1.0.0-py3.8.egg/unimol_tools/weights/mol_pre_no_h_220816.pt\n",
      "Checking existing SMILES: 100%|██████████| 1240/1240 [00:00<00:00, 100981.22it/s]\n",
      "Processing new SMILES in batches:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-25 13:46:14 | unimol_tools/data/conformer.py | 90 | INFO | Uni-Mol(QSAR) | Start generating conformers...\n",
      "1240it [00:12, 101.02it/s]\n",
      "2025-03-25 13:46:27 | unimol_tools/data/conformer.py | 94 | INFO | Uni-Mol(QSAR) | Failed to generate conformers for 0.00% of molecules.\n",
      "2025-03-25 13:46:27 | unimol_tools/data/conformer.py | 96 | INFO | Uni-Mol(QSAR) | Failed to generate 3d conformers for 0.24% of molecules.\n",
      "100%|██████████| 39/39 [00:11<00:00,  3.34it/s]\n",
      "Processing new SMILES in batches: 100%|██████████| 1/1 [02:46<00:00, 166.83s/it]\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sys\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the path of 'unimol_tools' to Python's search path\n",
    "sys.path.append('./Uni-Mol-main/unimol_tools/unimol_tools')\n",
    "from unimol_tools import UniMolRepr\n",
    "\n",
    "# =========================\n",
    "# 1. Database operations\n",
    "# =========================\n",
    "\n",
    "def create_table(conn):\n",
    "    \"\"\"\n",
    "    Create (if it does not exist) the table 'molecule_atomic_embeddings' \n",
    "    in the database to store molecular representations.\n",
    "    :param conn: sqlite3.Connection object.\n",
    "    \"\"\"\n",
    "    conn.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS molecule_atomic_embeddings (\n",
    "            smiles TEXT PRIMARY KEY,\n",
    "            embedding BLOB,\n",
    "            atomic_embedding BLOB\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "def store_molecule_embedding(conn, smiles, embedding, atomic_embedding):\n",
    "    \"\"\"\n",
    "    Store the UniMol CLS-level representation and atomic-level representation\n",
    "    of a molecule into the database.\n",
    "    :param conn: sqlite3.Connection object.\n",
    "    :param smiles: String, SMILES of the molecule.\n",
    "    :param embedding: ndarray, the CLS (global) representation of the molecule.\n",
    "    :param atomic_embedding: ndarray, the atomic-level representations of the molecule.\n",
    "    \"\"\"\n",
    "    embedding_buffer = io.BytesIO()\n",
    "    atomic_buffer = io.BytesIO()\n",
    "\n",
    "    # Save the numpy arrays into in-memory buffers\n",
    "    np.save(embedding_buffer, embedding)\n",
    "    np.save(atomic_buffer, atomic_embedding)\n",
    "    embedding_buffer.seek(0)\n",
    "    atomic_buffer.seek(0)\n",
    "\n",
    "    # Store them in the database as BLOBs\n",
    "    conn.execute('''\n",
    "        INSERT OR REPLACE INTO molecule_atomic_embeddings (smiles, embedding, atomic_embedding)\n",
    "        VALUES (?, ?, ?)\n",
    "    ''', (\n",
    "        smiles,\n",
    "        sqlite3.Binary(embedding_buffer.read()),\n",
    "        sqlite3.Binary(atomic_buffer.read())\n",
    "    ))\n",
    "    conn.commit()\n",
    "\n",
    "def get_molecule_embedding(conn, smiles):\n",
    "    \"\"\"\n",
    "    Retrieve the UniMol CLS-level representation and atomic-level representation\n",
    "    for a given SMILES from the database.\n",
    "    :param conn: sqlite3.Connection object.\n",
    "    :param smiles: String, SMILES of the molecule.\n",
    "    :return: A tuple (embedding, atomic_embedding); returns (None, None) if not found.\n",
    "    \"\"\"\n",
    "    cursor = conn.execute('''\n",
    "        SELECT embedding, atomic_embedding\n",
    "        FROM molecule_atomic_embeddings\n",
    "        WHERE smiles=?\n",
    "    ''', (smiles,))\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    if result:\n",
    "        # result[0] -> embedding, result[1] -> atomic_embedding\n",
    "        return (\n",
    "            np.load(io.BytesIO(result[0]), allow_pickle=True),\n",
    "            np.load(io.BytesIO(result[1]), allow_pickle=True)\n",
    "        )\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# =========================\n",
    "# 2. Representation generation\n",
    "# =========================\n",
    "\n",
    "def process_smiles_batch(smiles_batch):\n",
    "    \"\"\"\n",
    "    Process a batch of SMILES strings, returning a list of (SMILES, CLS representation, atomic representation).\n",
    "    Use only for small batches or when needed; for large datasets, consider a single initialization of UniMolRepr.\n",
    "    :param smiles_batch: List of SMILES strings.\n",
    "    :return: A list of tuples (smiles, cls_repr, atomic_reprs).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This uses CPU mode by default; enable use_gpu=True if needed\n",
    "        clf = UniMolRepr(data_type='molecule', remove_hs=True)\n",
    "        unimol_repr = clf.get_repr(smiles_batch, return_atomic_reprs=True)\n",
    "\n",
    "        results = []\n",
    "        for i, smiles in enumerate(smiles_batch):\n",
    "            cls_repr = np.array(unimol_repr['cls_repr'][i])       # CLS representation\n",
    "            atomic_reprs = np.array(unimol_repr['atomic_reprs'][i])  # Atomic-level representation\n",
    "            results.append((smiles, cls_repr, atomic_reprs))\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process batch: {e}\")\n",
    "        return [None] * len(smiles_batch)\n",
    "\n",
    "def process_smiles_data(conn, smiles_list, batch_size=5000):\n",
    "    \"\"\"\n",
    "    Process the given list of SMILES in batches, generate UniMol representations, \n",
    "    and store them in the database.\n",
    "    :param conn: sqlite3.Connection object.\n",
    "    :param smiles_list: Iterable of SMILES strings.\n",
    "    :param batch_size: Integer, max number of SMILES to process in one batch. Adjust based on available memory.\n",
    "    \"\"\"\n",
    "    # Initialize UniMolRepr once (enable GPU if memory allows)\n",
    "    clf = UniMolRepr(data_type='molecule', remove_hs=True, use_gpu=True)\n",
    "\n",
    "    # Remove duplicates by converting to a set\n",
    "    unique_smiles_set = set(smiles_list)\n",
    "\n",
    "    # Collect SMILES strings not yet in the database\n",
    "    new_smiles_list = []\n",
    "    for smiles in tqdm(unique_smiles_set, desc=\"Checking existing SMILES\"):\n",
    "        # If the SMILES is not in the database, we include it for processing\n",
    "        if get_molecule_embedding(conn, smiles)[0] is None:\n",
    "            new_smiles_list.append(smiles)\n",
    "\n",
    "    # Process and store in batches\n",
    "    for i in tqdm(range(0, len(new_smiles_list), batch_size), desc=\"Processing new SMILES in batches\"):\n",
    "        batch = new_smiles_list[i : i+batch_size]\n",
    "        try:\n",
    "            unimol_repr = clf.get_repr(batch, return_atomic_reprs=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process batch: {e}\")\n",
    "            continue\n",
    "\n",
    "        for j, smiles in enumerate(batch):\n",
    "            cls_repr = np.array(unimol_repr['cls_repr'][j])\n",
    "            atomic_reprs = np.array(unimol_repr['atomic_reprs'][j])\n",
    "            store_molecule_embedding(conn, smiles, cls_repr, atomic_reprs)\n",
    "\n",
    "# =========================\n",
    "# 3. Main script\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect('./data/11betaHSD1/Uni-Mol_molecule_embeddings_no_h_11betaHSD1_ligands.db')\n",
    "    \n",
    "    # Create the table (if it does not exist)\n",
    "    create_table(conn)\n",
    "\n",
    "    # Load SMILES from a CSV file\n",
    "    df = pd.read_csv(\"./data/11betaHSD1/11betaHSD1.csv\")\n",
    "    smiles_data = df[\"SMILES\"].tolist()\n",
    "\n",
    "    # Process SMILES in batches and store to the database\n",
    "    process_smiles_data(conn, smiles_data, batch_size=5000)\n",
    "\n",
    "    # Close the database connection\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ankh Large protein embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing protein sequences: 100%|██████████| 500/500 [00:00<00:00, 1039.77it/s]\n",
      "Processing protein sequences: 100%|██████████| 500/500 [00:00<00:00, 1044.36it/s]\n",
      "Processing protein sequences: 100%|██████████| 240/240 [00:00<00:00, 1060.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1. Imports and Environment Setup\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import io\n",
    "import torch\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoConfig, AutoTokenizer, T5EncoderModel\n",
    "\n",
    "\n",
    "# Select device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# 2. Function Definitions\n",
    "# =========================\n",
    "\n",
    "def read_data_in_chunks(file_path, chunk_size):\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        yield chunk\n",
    "\n",
    "def create_table(conn):\n",
    "    conn.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS protein_embeddings (\n",
    "        protein_sequence TEXT PRIMARY KEY,\n",
    "        embedding BLOB\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "def store_embedding(conn, protein_sequence, embedding):\n",
    "    buffer = io.BytesIO()\n",
    "    np.save(buffer, embedding.cpu().numpy())\n",
    "    buffer.seek(0)\n",
    "    conn.execute('''\n",
    "    INSERT OR REPLACE INTO protein_embeddings (protein_sequence, embedding)\n",
    "    VALUES (?, ?)\n",
    "    ''', (protein_sequence, sqlite3.Binary(buffer.read())))\n",
    "    conn.commit()\n",
    "\n",
    "def get_protein_embedding(conn, protein_sequence):\n",
    "    cursor = conn.execute('''\n",
    "    SELECT embedding FROM protein_embeddings WHERE protein_sequence=?\n",
    "    ''', (protein_sequence,))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        return np.load(io.BytesIO(result[0]), allow_pickle=True)\n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# 3. Main Execution\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths and parameters\n",
    "    # Path to the pre-trained Ankh model \n",
    "    model_path = \"./model/Ankh_Large/Ankh_Large_model.pth\"\n",
    "    data_path = \"./data/11betaHSD1/11betaHSD1.csv\"\n",
    "    db_path = \"./data/11betaHSD1/Ankh_Large_target_protein_embeddings_11betaHSD1.db\"\n",
    "    max_length = 1200\n",
    "    chunk_size = 500\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = T5EncoderModel.from_pretrained(model_path, config=config)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    create_table(conn)\n",
    "\n",
    "    # Process protein sequences in chunks\n",
    "    for df_chunk in read_data_in_chunks(data_path, chunk_size):\n",
    "        for i in tqdm(range(len(df_chunk)), desc=\"Processing protein sequences\"):\n",
    "            original_seq = df_chunk.iloc[i]['Protein']\n",
    "\n",
    "            if get_protein_embedding(conn, original_seq) is not None:\n",
    "                continue\n",
    "\n",
    "            seq = original_seq[:1200] if len(original_seq) > 1200 else original_seq\n",
    "            tokenized = tokenizer.batch_encode_plus(\n",
    "                [list(seq)],\n",
    "                add_special_tokens=True,\n",
    "                padding=True,\n",
    "                is_split_into_words=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            input_ids = tokenized['input_ids'].to(device)\n",
    "            attention_mask = tokenized['attention_mask'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                encoder_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            token_repr = encoder_outputs.last_hidden_state\n",
    "\n",
    "            if token_repr.shape[1] > max_length:\n",
    "                token_repr = token_repr[:, :max_length, :]\n",
    "\n",
    "            store_embedding(conn, original_seq, token_repr.squeeze(0))\n",
    "\n",
    "            del token_repr, encoder_outputs, input_ids, attention_mask, tokenized, seq, original_seq\n",
    "            gc.collect()\n",
    "\n",
    "    # Close the database\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProtT5 protein embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing protein sequences: 100%|██████████| 1000/1000 [00:02<00:00, 483.28it/s]\n",
      "Processing protein sequences: 100%|██████████| 240/240 [00:00<00:00, 1295.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1. Imports and Environment Setup\n",
    "# =========================\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import io\n",
    "import torch\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "\n",
    "# Select device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# 2. Function Definitions\n",
    "# =========================\n",
    "\n",
    "def read_data_in_chunks(file_path, chunk_size):\n",
    "    \"\"\"Generator function to read data in chunks from a CSV file.\"\"\"\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        yield chunk\n",
    "\n",
    "def create_table(conn):\n",
    "    \"\"\"Creates the protein_embeddings table in the SQLite database if it doesn't exist.\"\"\"\n",
    "    conn.execute(''' \n",
    "    CREATE TABLE IF NOT EXISTS protein_embeddings (\n",
    "        protein_sequence TEXT PRIMARY KEY,\n",
    "        embedding BLOB\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "def store_embedding(conn, protein_sequence, embedding):\n",
    "    \"\"\"Stores protein sequence embeddings into the SQLite database.\"\"\"\n",
    "    buffer = io.BytesIO()\n",
    "    np.save(buffer, embedding.cpu().numpy())\n",
    "    buffer.seek(0)\n",
    "    conn.execute('''\n",
    "    INSERT OR REPLACE INTO protein_embeddings (protein_sequence, embedding)\n",
    "    VALUES (?, ?)\n",
    "    ''', (protein_sequence, sqlite3.Binary(buffer.read())))\n",
    "    conn.commit()\n",
    "\n",
    "def get_protein_embedding(conn, protein_sequence):\n",
    "    \"\"\"Retrieves the protein embedding from the database.\"\"\"\n",
    "    cursor = conn.execute('''\n",
    "    SELECT embedding FROM protein_embeddings WHERE protein_sequence=?\n",
    "    ''', (protein_sequence,))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        return np.load(io.BytesIO(result[0]), allow_pickle=True)\n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# 3. Main Execution\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths and parameters\n",
    "    model_path = \"./ProstT5/Rostlab/ProstT5\"\n",
    "    data_path = \"./data/11betaHSD1/11betaHSD1.csv\"\n",
    "    db_path = \"./data/11betaHSD1/ProtT5_target_protein_embeddings_11betaHSD1.db\"\n",
    "    # model_path = \"/mnt/USR_DATA/ChenGeng/Project/CPI_baseline_model/A_wenzhang/ProstT5/Rostlab/ProstT5\"\n",
    "    # data_path = \"/mnt/USR_DATA/ChenGeng/Project/CPI_baseline_model/A_wenzhang/ankh/5aga/HitScreen/data/11betaHSD1/11betaHSD1.csv\"\n",
    "    # db_path = \"/mnt/USR_DATA/ChenGeng/Project/CPI_baseline_model/A_wenzhang/ankh/5aga/HitScreen/data/11betaHSD1/ProtT5_target_protein_embeddings_11betaHSD1.db\"\n",
    "    \n",
    "    max_length = 1200\n",
    "    chunk_size = 1000\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path, do_lower_case=False)\n",
    "    model = T5EncoderModel.from_pretrained(model_path).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    create_table(conn)\n",
    "\n",
    "    # Process protein sequences in chunks\n",
    "    for df_chunk in read_data_in_chunks(data_path, chunk_size):\n",
    "        for i in tqdm(range(len(df_chunk)), desc=\"Processing protein sequences\"):\n",
    "            original_seq = df_chunk.iloc[i]['Protein']\n",
    "\n",
    "            # Skip if the embedding already exists in the database\n",
    "            if get_protein_embedding(conn, original_seq) is not None:\n",
    "                continue\n",
    "\n",
    "            # Truncate the sequence if it's too long\n",
    "            seq = original_seq[:1200] if len(original_seq) > 1200 else original_seq\n",
    "            protein_sequences = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", seq)))]\n",
    "            protein_sequences = [\"<AA2fold> \" + s for s in protein_sequences]\n",
    "\n",
    "            # Tokenize the sequence\n",
    "            tokenized = tokenizer.batch_encode_plus(\n",
    "                protein_sequences,\n",
    "                add_special_tokens=True,\n",
    "                padding=\"longest\",\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            # Generate embeddings\n",
    "            with torch.no_grad():\n",
    "                embedding_repr = model(input_ids=tokenized.input_ids, attention_mask=tokenized.attention_mask)\n",
    "            \n",
    "            # Extract embedding and remove padding\n",
    "            emb_0 = embedding_repr.last_hidden_state[0, 1:len(protein_sequences[0].split()) - 1]\n",
    "\n",
    "            # Store the embedding in the database\n",
    "            store_embedding(conn, original_seq, emb_0)\n",
    "\n",
    "            # Clear memory\n",
    "            del emb_0, embedding_repr, tokenized, protein_sequences, seq, original_seq\n",
    "            gc.collect()\n",
    "\n",
    "    # Close the database connection\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data: 100%|██████████| 1000/1000 [00:01<00:00, 685.17it/s]\n",
      "Preprocessing data: 100%|██████████| 240/240 [00:00<00:00, 1702.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1. Imports and Environment Setup\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import io\n",
    "import torch\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import esm\n",
    "\n",
    "\n",
    "# Select device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =========================\n",
    "# 2. Function Definitions\n",
    "# =========================\n",
    "\n",
    "def read_data_in_chunks(file_path, chunk_size):\n",
    "    \"\"\"Generator function to read data in chunks from a CSV file.\"\"\"\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        yield chunk\n",
    "\n",
    "def create_table(conn):\n",
    "    \"\"\"Creates the protein_embeddings table in the SQLite database if it doesn't exist.\"\"\"\n",
    "    conn.execute(''' \n",
    "    CREATE TABLE IF NOT EXISTS protein_embeddings (\n",
    "        protein_sequence TEXT PRIMARY KEY,\n",
    "        embedding BLOB\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "def store_embedding(conn, protein_sequence, embedding):\n",
    "    \"\"\"Stores protein sequence embeddings into the SQLite database.\"\"\"\n",
    "    buffer = io.BytesIO()\n",
    "    np.save(buffer, embedding.cpu().numpy())\n",
    "    buffer.seek(0)\n",
    "    conn.execute('''\n",
    "    INSERT OR REPLACE INTO protein_embeddings (protein_sequence, embedding)\n",
    "    VALUES (?, ?)\n",
    "    ''', (protein_sequence, sqlite3.Binary(buffer.read())))\n",
    "    conn.commit()\n",
    "\n",
    "def get_protein_embedding(conn, protein_sequence):\n",
    "    \"\"\"Retrieves the protein embedding from the database.\"\"\"\n",
    "    cursor = conn.execute('''\n",
    "    SELECT embedding FROM protein_embeddings WHERE protein_sequence=?\n",
    "    ''', (protein_sequence,))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        return np.load(io.BytesIO(result[0]), allow_pickle=True)\n",
    "    return None\n",
    "\n",
    "def process_data(file_path, conn, batch_converter, model, max_length=1200):\n",
    "    \"\"\"Processes protein sequences, generates embeddings, and stores them in the database.\"\"\"\n",
    "    for df_chunk in read_data_in_chunks(file_path, chunk_size=1000):\n",
    "        for i in tqdm(range(len(df_chunk)), desc=\"Preprocessing data\"):\n",
    "            original_v_p = df_chunk.iloc[i]['Protein']\n",
    "            \n",
    "            # Skip if the embedding already exists in the database\n",
    "            if get_protein_embedding(conn, original_v_p) is not None:\n",
    "                continue\n",
    "            \n",
    "            # Truncate the sequence if it exceeds max_length\n",
    "            v_p = original_v_p[:1200] if len(original_v_p) > 1200 else original_v_p\n",
    "            \n",
    "            # Convert the protein sequence into an embedding\n",
    "            data = [(\"protein1\", v_p)]\n",
    "            _, _, batch_tokens = batch_converter(data)\n",
    "            with torch.no_grad():\n",
    "                results = model(batch_tokens, repr_layers=[30], return_contacts=True)\n",
    "                token_representations = results[\"representations\"][30]\n",
    "            \n",
    "            # Remove the batch dimension and process the embedding\n",
    "            token_representations = token_representations.squeeze(0)\n",
    "            \n",
    "            # Truncate to max_length if necessary\n",
    "            if token_representations.shape[0] > max_length:\n",
    "                token_representations = token_representations[:max_length]\n",
    "            \n",
    "            # Store the embedding in the database\n",
    "            store_embedding(conn, original_v_p, token_representations)\n",
    "\n",
    "            # Free up memory\n",
    "            del token_representations, results, batch_tokens, data, v_p, original_v_p\n",
    "            gc.collect()\n",
    "\n",
    "# =========================\n",
    "# 3. Main Execution\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define file paths\n",
    "    data_path = \"/mnt/USR_DATA/ChenGeng/Project/CPI_baseline_model/A_wenzhang/ankh/5aga/HitScreen/data/11betaHSD1/11betaHSD1.csv\"\n",
    "    db_path = \"/mnt/USR_DATA/ChenGeng/Project/CPI_baseline_model/A_wenzhang/ankh/5aga/HitScreen/data/11betaHSD1/ESM-2_150M_target_protein_embeddings_11betaHSD1.db\"\n",
    "\n",
    "    # Load ESM model and alphabet\n",
    "    model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()\n",
    "\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    create_table(conn)\n",
    "\n",
    "    # Process protein sequences and generate embeddings\n",
    "    process_data(data_path, conn, batch_converter, model)\n",
    "\n",
    "    # Close the database connection\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
